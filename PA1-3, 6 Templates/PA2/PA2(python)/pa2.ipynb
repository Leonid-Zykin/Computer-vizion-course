{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authors\n",
    "\n",
    "*The work can be done in groups of up to 3 students. Please complete the following fields with your group number and list your names along with ISU ID numbers.*\n",
    "\n",
    "---\n",
    "> **Stream:** *[RZZZ] Technical Vision*\n",
    "> 1. Name, ISU id\n",
    "> 2. Name, ISU id\n",
    "> 3. Name, ISU id\n",
    "---\n",
    "\n",
    "The task and guidelines were prepared by Andrei Zhdanov And Sergei Shavetov from ITMO University in 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Assignment No 2. Images Geometric Transformations\n",
    "\n",
    "Studying of mapping main types and using geometric transforma- tions for spatial image correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Simplest geometric transformations\n",
    "\n",
    "*Select an arbitrary image. Perform linear and nonlinear transformations over it (Eucidean, affine and projective mappings).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images geometric transformations imply a spatial change in the location of pixels set with integer coordinates $(x,y)$ to another set with coordinates $(x',y')$, and the pixels intensity doesn't changes. In two-dimensional plane geometric transformations, as a rule, Euclidean space is used $\\mathbf{P}^2$ with an orthonormal Cartesian coordinate system. In this case, the image pixel corresponds to a pair of Cartesian coordinates, which are interpreted as a two-dimensional vector represented by a segment from a point $(0,0)$ to a point $X_i=(x_i,y_i)$. Two-dimensional transformations on a plane can be represented as the points movement corresponding to a pixels plurality.\n",
    "\n",
    "For generality with further transformations, we will use *homogeneous coordinates*. If homogeneous coordinates of a point are multiplied by a non-zero scalar then the resulting coordinates represent the same point. Due to this the required coordinates number to represent points is always one more than the space dimension $\\mathbf{P}^n$, in which these coordinates are used. For example, to represent a point $X=(x,y)$ on a plane in two-dimensional space $\\mathbf{P}^2$ three coordinates are required $X=(x,y,w)$. Let's illustrate this with the following example:\n",
    "\n",
    "$$\n",
    "\t\\begin{bmatrix} x' \\\\ y' \\\\ w \\end{bmatrix} = w\n",
    "\t\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\n",
    "\t\\Leftrightarrow \n",
    "\t\\begin{bmatrix} x' & y' & w \\end{bmatrix} =\n",
    "\t\\begin{bmatrix} x & y & 1 \\end{bmatrix}w\n",
    "$$\n",
    "\n",
    "where $w$ is an arbitrary scalar factor, $x=\\dfrac{x'}{w}$, $y=\\dfrac{y'}{w}$.\n",
    "\n",
    "Any linear transformation of the plane can be described using triples of homogeneous coordinates and third order matrices. Thus, geometric transformations are matrix transformations, and the sets pixel coordinates of the transformed and original images are related by the following matrix relation or in a row form  $X'=XT$, or in a column form $X'=T^{\\mathrm{T}}X$. Let's rewrite these relations:\n",
    "\n",
    "$$\n",
    "\t\\begin{bmatrix} x' & y' & w' \\end{bmatrix} = \n",
    "\t\\begin{bmatrix} x & y & w \\end{bmatrix}\n",
    "\t\\cdot\n",
    "\t\\begin{bmatrix} \n",
    "\t\tA & D & G \n",
    "\t\t\\\\ B & E & H \n",
    "\t\t\\\\ C & F & I\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Leftrightarrow\n",
    "\t\\begin{bmatrix} x' \\\\ y' \\\\ w' \\end{bmatrix} = \n",
    "\t\\begin{bmatrix} \n",
    "\t\tA & B & C \n",
    "\t\t\\\\ D & E & F \n",
    "\t\t\\\\ G & H & I\n",
    "\t\\end{bmatrix} \\cdot\n",
    "\t\\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Point with Cartesian coordinates $(x,y)$ in homogeneous coordinates is written as $(x,y,1)$:\n",
    "\n",
    "$$\n",
    "\t\\begin{bmatrix} x' & y' & 1 \\end{bmatrix} = \n",
    "\t\\begin{bmatrix} x & y & 1 \\end{bmatrix}\n",
    "\t\\cdot\n",
    "\t\\begin{bmatrix} \n",
    "\t\tA & D & 0 \n",
    "\t\t\\\\ B & E & 0 \n",
    "\t\t\\\\ C & F & 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\t\\Leftrightarrow\n",
    "\t\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \n",
    "\t\\begin{bmatrix} \n",
    "\t\tA & B & C \n",
    "\t\t\\\\ D & E & F \n",
    "\t\t\\\\ 0 & 0 & 1\n",
    "\t\\end{bmatrix} \\cdot\n",
    "\t\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, the transformation formula  can be rewritten as following system equations:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tx'=Ax+By+C, \n",
    "\t\t\\\\ y'=Dx+Ey+F\n",
    "\t\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenCV affine transformation matrices**\n",
    "\n",
    "OpenCV used $2D$ vectores as a pixel coordinates and stores transformation matrix in a form of $2 \\times 2$ transformation matrix and a shift vector. Also, vectors are stored columns, so the above tranformation folrmula should be rewritten as follows:\n",
    "\n",
    "$$\n",
    "\t\\begin{bmatrix} \n",
    "\t\tx' \\\\ y'\n",
    "\t\\end{bmatrix} =\n",
    "\tT \\cdot\n",
    "\t\\begin{bmatrix} \n",
    "\t\tx \\\\ y\n",
    "\t\\end{bmatrix}\n",
    "  + V\n",
    "$$\n",
    "\n",
    "Moreover, the matrix $T$ and vector $V$ are comined in a single $2 \\times 3$ matrix to form a single OpenCV transformation matrix:\n",
    "\n",
    "$$\n",
    "  T' = \n",
    "\t\\begin{bmatrix} \n",
    "\t\tT & V\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To match this definition with the one given above, we have to set our matrix and vector as following:\n",
    "\n",
    "$$\n",
    "  T = \n",
    "\t\\begin{bmatrix} \n",
    "\t\tA & B \\\\\n",
    "\t\tD & E\n",
    "\t\\end{bmatrix}, \n",
    "  V = \n",
    "\t\\begin{bmatrix} \n",
    "\t\tC \\\\\n",
    "\t\tF\n",
    "\t\\end{bmatrix},\n",
    "\tT' =\n",
    "\t\\begin{bmatrix} \n",
    "\t\tA & B & C\\\\\n",
    "\t\tD & E &F\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then after substitution we get:\n",
    "\n",
    "$$\n",
    "\t\\begin{bmatrix} \n",
    "\t\tx' \\\\\n",
    "\t\ty'\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix} \n",
    "\t\tA & B \\\\\n",
    "\t\tD & E\n",
    "\t\\end{bmatrix} \n",
    "  \\cdot  \n",
    "\t\\begin{bmatrix} \n",
    "\t\tx \\\\\n",
    "\t\ty\n",
    "\t\\end{bmatrix}\n",
    "  +\n",
    "\t\\begin{bmatrix} \n",
    "\t\tC \\\\\n",
    "\t\tF\n",
    "\t\\end{bmatrix}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Preparation\n",
    "\n",
    "First, we need to add some imports for OpenCV to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cv2 as cv\n",
    "import numpy\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we will import `ShowImages()` function from the first practical assignment to use it here. It is placed in `pa_utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pa_utils import ShowImages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read and display an image\n",
    "\n",
    "Now let's open our image which we will use during the current task. We will open it in BGR and convert to grayscale. We will use previously introduced functions for image display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image from file in BGR\n",
    "fn = \"images/lena_color.png\"\n",
    "I = cv.imread(fn, cv.IMREAD_COLOR)\n",
    "if not isinstance(I, np.ndarray) or I.data == None:\n",
    "  print(\"Error reading file \\\"{}\\\"\".format(fn))\n",
    "else:\n",
    "  # Display it\n",
    "  ShowImages([(\"Source image\", I)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Linear transformations\n",
    "\n",
    "**Linear transformation** (also called **Linear mapping**) --- is a mapping that preserves the infinitesimal figures shape and the angles between curves at their intersection points. The main linear mapping are Euclidean transformations. These transformations include shift, flip, uniform scaling, and rotation. Linear transformations are a affine transformations subset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Image shift\n",
    "\n",
    "The simplest image transformation is an image shift. System equations and coordinates transformation matrix $T$ in case of image *shift* $A=E=1$, $B=D=0$ take the form:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tx'=x+C \\\\\n",
    "\t\ty'=y+F\n",
    "\t\\end{cases} \n",
    "$$\n",
    "\n",
    "Which can be also written in the vector form\n",
    "\n",
    "$$\n",
    "\t\\begin{bmatrix} \n",
    "\t\tx' & y' & 1\n",
    "\t\\end{bmatrix} =\n",
    "\t\\begin{bmatrix} \n",
    "\t\tx & y & 1\n",
    "\t\\end{bmatrix}\n",
    "\tT\n",
    "$$\n",
    "\n",
    "And gives us a very simple transformation matrix\n",
    "\n",
    "$$\n",
    "\tT =\n",
    "\t\\begin{bmatrix} \n",
    "\t\t1 & 0 & 0 \n",
    "\t\t\\\\ 0 & 1 & 0\n",
    "\t\t\\\\ C & F & 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $C$ and $F$ are the shift values along axes $Ox$ and $Oy$ correspondingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use OpenCV, we should define this matrix in the following form:\n",
    "\n",
    "$$\n",
    "\tT =\n",
    "\t\\begin{bmatrix} \n",
    "\t\t1 & 0 & C \n",
    "\t\t\\\\ 0 & 1 & F\n",
    "\t\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "In OpenCV implementation for the Python programming language the transformation matrix is a simple `numpy` array with dimensions $3 \\times 2$, so it can be created by calling the `numpy.float32()` function. Then, the `cv2.warpAffine()` function should be used to perform the affine transformation. Its first argument is an image to transform, the second one is the transformation matrix and the third one is a resolution to be used when creating the resulting image. It returns a transformed image as a result. As the image resolution may be changed during the image transformation, this could be accounted and given as a third image parameter. In case of image shift, the resolution is changed by the shift value, so we will account this by increasing the image size.\n",
    "\n",
    "**Please note** Since numpy arrays are used to store images, so all methods that are applicable to numpy arrays can be executed for images as well. However this results in a little problem as images and matrices has a different order of width (number of columns) and height (number of rows) definition way. In general, image is defined by $width \\times height$, however matrix is defined by $rows \\times columns$, so in the image shape parameter `I.shape` the image resolution is stored as numbers of rows and columns, but OpenCV functions, e.g, the transformation function, the resolution is defined as width by height, so it should get the numbers of columns and number of rows.\n",
    "\n",
    "First, let's try shifting an image without setting the desired transformed image resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation matrix and call cv.warpAffine\n",
    "shift_x, shift_y = (50, 100)\n",
    "T = np.float32([[1, 0, shift_x], \n",
    "                [0, 1, shift_y]])\n",
    "Ishift = cv.warpAffine(I, T, I.shape[0:2])\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), \n",
    "            (\"Image shift\", Ishift)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some part of the image was lost due to to fact that transformed image data now moved outside of the image canvas. To correct it we should know the desired transformed image resolution and pass the correct value as a third argument to the `cv2.warpAffine()` function. The transformed image resolution is a tuple for the transformed image width and height. In current case it should be equal to $(width + 50, height  + 100)$.\n",
    "\n",
    "Let's account for it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation matrix and call cv.warpAffine\n",
    "shift_x, shift_y = (50, 100)\n",
    "T = np.float32([[1, 0, shift_x], \n",
    "                [0, 1, shift_y]])\n",
    "Ishift2 = cv.warpAffine(I, T, (I.shape[1] + shift_x, I.shape[0] + shift_y))\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), \n",
    "            (\"Image shift\", Ishift2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see now image is shifted and due to a resolution change no image information is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Shift an image to the left and up by $(-50, -100)$. Don't forget to change the image resolution when transforming an image. It should be decreased by the shift vector.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution for part 1 here\n",
    "# Create a matrix for shift\n",
    "# Calculate the transformed image resolution\n",
    "# Apply the transformation with cv2.warpAffine() function \n",
    "# Display the result\n",
    "Ishift3 = np.zeros_like(I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Image flip\n",
    "\n",
    "The transformation equation and coordinates transformation matrix $T$ parameters in case of *image flip* around the axis $Ox$ (along axis $oY$) are $A=1$, $E=-1$, $B=C=D=F=0$. Then equations and matrix will take the form:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tx'=x \\\\\n",
    "\t\ty'=-y\n",
    "\t\\end{cases} \n",
    "\t\\Rightarrow\n",
    "\tT=\n",
    "\t\\begin{bmatrix} \n",
    "\t\t1 & 0 & 0 \n",
    "\t\t\\\\ 0 & -1 & 0\n",
    "\t\t\\\\ 0 & 0 & 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In case of OpenCV the transformation matrix should be defined as:\n",
    "\n",
    "$$\n",
    "\tT =\n",
    "\t\\begin{bmatrix} \n",
    "\t\t1 & 0 & 0 \n",
    "\t\t\\\\ 0 & -1 & 0\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip around Ox\n",
    "T = np.float32([[1, 0, 0], \n",
    "                [0, -1, 0]])\n",
    "Iflip_y = cv.warpAffine(I, T, I.shape[:2][::-1])\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), \n",
    "            (\"Image flip Y\", Iflip_y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, we got a black image. This happened because after flipping an image, all pixel coordinates became negative and got outside of the image area. So, we have to shift them to the image area by shifting our image right by $rows - 1$:\n",
    "\n",
    "$$\n",
    "\tT =\n",
    "\t\\begin{bmatrix} \n",
    "\t\t1 & 0 & 0 \n",
    "\t\t\\\\ 0 & -1 & I.rows - 1\n",
    "\t\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip around Ox\n",
    "T = np.float32([[1, 0, 0], \n",
    "                [0, -1, I.shape[0] - 1]])\n",
    "Iflip_y_2 = cv.warpAffine(I, T, I.shape[:2][::-1])\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), \n",
    "            (\"Correct image flip Y\", Iflip_y_2)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV also provides a function for fast flipping an image called `flip()`. It takes one parameter in addition to an image to flip. The value $0$ means flipping along $Ox$ axis, $1$ is for flipping along $Oy$ axis, and $-1$ is to flip along both axes. \n",
    "\n",
    "Let's try doing the same flip with OpenCV built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip around Ox\n",
    "Iflip_y_ocv = cv.flip(I, 0)\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), \n",
    "            (\"Built-im image flip Y\", Iflip_y_ocv)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Flip an image along both axis ($Ox$ and $Oy$). Implement two solutions:\n",
    ">\n",
    "> 1. Create the transformation matrix and apply it with `cv2.warpAffine()` function (you may either create a single transformation matrix or apply transformation two times: for $Ox$ and $Oy$ axis);\n",
    "> 2. Use the OpenCV built-in `cv2.flip()` function.\n",
    ">\n",
    "> Compare the results, they should match.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution for part 1 here\n",
    "# Create a matrix for flip (either one matrix for single flip or two matrices for flip around Ox and around Oy)\n",
    "# Apply the transformation with cv2.warpAffine() function\n",
    "# Display the result\n",
    "Iflip_xy = np.zeros_like(I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution for part 2 here\n",
    "# Flip the image with cv2.flip() function\n",
    "# Display the result\n",
    "Iflip_xy_ocv = np.zeros_like(I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your rotation results match? Don't forget to check it. There might be a little difference due to a different floating point values processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Uniform image scale\n",
    "\n",
    "The transformation equation and coordinates transformation matrix $T$ parameters in case of *image scale* by $\\alpha$ along $oX$ axis and $\\beta$ along $oY$ axis are $A=\\alpha$, $E=\\beta$, $B=C=D=F=0$. Then equations and matrix will take the form:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tx'={\\alpha}x, \\alpha > 0 \\\\\n",
    "\t\ty'={\\beta}y, \\beta > 0\n",
    "\t\\end{cases}\n",
    "\t\\Rightarrow \n",
    "\tT=\n",
    "\t\\begin{bmatrix} \n",
    "\t\t\\alpha & 0 & 0 \n",
    "\t\t\\\\ 0 & \\beta & 0\n",
    "\t\t\\\\ 0 & 0 & 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If $\\alpha<1$ and $\\beta<1$, then the image decreases in size, if $\\alpha>1$ and $\\beta>1$ then it increases. If $\\alpha \\ne \\beta$, then the proportions will not be the same in width and height. In general, this mapping will be affine, not linear.\n",
    "\n",
    "In case of using OpenCV libraries the transformation matrix should be defined as:\n",
    "\n",
    "$$\n",
    "\tT =\n",
    "\t\\begin{bmatrix} \n",
    "\t\t\\alpha & 0 & 0 \n",
    "\t\t\\\\ 0 & \\beta & 0\n",
    "\t\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "When resizing an image we should also change the image size as well, so the new image size will be $(width \\cdot \\alpha \\times height \\cdot \\beta)$. This can be accounted by the third argument of the `cv2.warpAffine()` function which takes a tuple for the transformed image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image scale\n",
    "scale = (0.5, 0.5)\n",
    "T = np.float32([[scale[0], 0, 0], \n",
    "                [0, scale[1], 0]])\n",
    "Iscale = cv.warpAffine(I, T, (int(I.shape[1] * scale[0]), int(I.shape[0] * scale[1]))) \n",
    "\n",
    "# Display it (here we have to show axes to see the image resize results)\n",
    "ShowImages([(\"Source image\", I), \n",
    "            (\"Image scale\", Iscale)], 2, hide_axes = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV also provides function called `resize()` to resize an image in a single command. We may use it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image scale\n",
    "scale = (0.5, 0.5)\n",
    "Iscale_ocv = cv.resize(I, None, fx = scale[0], fy = scale[1], interpolation = cv.INTER_CUBIC)\n",
    "\n",
    "# Display it (here we have to show axes to see the image resize results)\n",
    "ShowImages([(\"Source image\", I), \n",
    "            (\"Built-in image scale\", Iscale_ocv)], 2, hide_axes = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Scale an image to upscale it by one and a half times (scale factor $1.5$ along both axis). Implement two solutions:\n",
    ">\n",
    "> 1. Create the transformation matrix and apply it with `cv2.warpAffine()` function;\n",
    "> 2. Use the OpenCV built-in `scale()` function.\n",
    ">\n",
    "> Compare the results, they should match. Don't forget that this transformation should change the image size.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution for part 1 here\n",
    "# Create matrix for scale\n",
    "# Apply transformation with cv2.warpAffine() function\n",
    "# Display the result\n",
    "Iscale15 = np.zeros_like(I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution for part 2 here\n",
    "# Scale the image with cv2.scale() function\n",
    "# Display the result\n",
    "Iscale15_ocv = np.zeros_like(I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your rotation results match? Don't forget to check it. There might be a little difference due to a different floating point values processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Image rotation\n",
    "\n",
    "The transformation equation and coordinates transformation matrix $T$ parameters in case of *image rotation* by $\\varphi$ degree clockwise around the coordinate system center  are $A=\\cos \\varphi$, $B=-\\sin \\varphi$, $D=\\sin \\varphi$, $E=\\cos \\varphi$, $C=F=0$. Then equations and matrix will take the form:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tx'=x \\cos \\varphi - y \\sin \\varphi \\\\\n",
    "\t\ty'=x \\sin \\varphi + y \\cos \\varphi\n",
    "\t\\end{cases}\n",
    "\t\\Rightarrow \n",
    "\tT=\n",
    "\t\\begin{bmatrix} \n",
    "\t\t{\\cos \\varphi} & {-\\sin \\varphi} & 0 \n",
    "\t\t\\\\ {\\sin \\varphi} & {\\cos \\varphi} & 0\n",
    "\t\t\\\\ 0 & 0 & 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If $\\varphi = 90^{\\circ}$, then $\\cos \\varphi = 0$ and $\\sin \\varphi = 1$ and transformation equation take the form:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tx'=-y \\\\\n",
    "\t\ty'=x\n",
    "\t\\end{cases} \n",
    "\t\\Rightarrow \n",
    "\tT=\n",
    "\t\\begin{bmatrix} \n",
    "\t\t0 & -1 & 0 \n",
    "\t\t\\\\ 1 & 0 & 0\n",
    "\t\t\\\\ 0 & 0 & 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of using OpenCV libraries the transformation matrix should be defined as:\n",
    "\n",
    "$$\n",
    "\tT =\n",
    "\t\\begin{bmatrix} \n",
    "\t\t{\\cos \\varphi} & {- \\sin \\varphi} & 0 \n",
    "\t\t\\\\ {\\sin \\varphi} & {\\cos \\varphi} & 0\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image rotation\n",
    "phi = 17.0 * math.pi / 180 # angle should be defined in radians\n",
    "T = np.float32([[math.cos(phi),  -math.sin(phi), 0], \n",
    "                [math.sin(phi),   math.cos(phi), 0]])\n",
    "Irotate = cv.warpAffine(I, T, I.shape[:2][::-1]) \n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), (\"Image rotation\", Irotate)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is really rotated around the coordinate system center which is top left corner, however in general we would like to rotate it around an arbitrary point. To do it, the rotations should be defined as a set of image transformations which are:\n",
    "\n",
    "1. Shift an image so that the rotation point is in the top left corner;\n",
    "2. Rotate it with a given angle;\n",
    "3. Shift it back. \n",
    "\n",
    "This can be written in a matrix form and then executed as a single affine transformation:\n",
    "\n",
    "$$\n",
    "\tT_{shift} =\n",
    "\t\\begin{bmatrix} \n",
    "\t\t1 & 0 & -(I.cols - 1) / 2\n",
    "\t\t\\\\ 0 & 1 & -(I.rows - 1) / 2\n",
    "\t\t\\\\ 0 & 0 & 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\tT_{rotate} =\n",
    "\t\\begin{bmatrix} \n",
    "\t\t{\\cos \\varphi} & {- \\sin \\varphi} & 0 \n",
    "\t\t\\\\ {\\sin \\varphi} & {\\cos \\varphi} & 0\n",
    "\t\t\\\\ 0 & 0 & 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\tT_{shiftback} =\n",
    "\t\\begin{bmatrix} \n",
    "\t\t1 & 0 & (I.cols - 1) / 2\n",
    "\t\t\\\\ 0 & 1 & (I.rows - 1) / 2\n",
    "\t\t\\\\ 0 & 0 & 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here $T_{shift}$ matrix is a forward shift to match $(0, 0)$ with image center, $T_{rotate}$ is rotation matrix, and $T_{shiftback}$ is backward shift to move image back to place after rotation. Then the final transformation matrix $T$ is calculated by multiplying these three transformation matrices and taking two first rows from it.\n",
    "\n",
    "$$T = T_{shiftback} \\times T_{rotate} \\times T_{shift}$$\n",
    "\n",
    "Let's implement it with OpenCV and rotate our image around its center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image rotation around image center\n",
    "phi = 17.0 * math.pi / 180 # angle should be defined in radians\n",
    "Tshift = np.float32([[1, 0, -(I.shape[1] - 1) / 2.0], \n",
    "                     [0, 1, -(I.shape[0] - 1) / 2.0], \n",
    "                     [0, 0, 1]])\n",
    "Trotate = np.float32([[math.cos(phi), -math.sin(phi), 0], \n",
    "                      [math.sin(phi),  math.cos(phi), 0], \n",
    "                      [0, 0, 1]])\n",
    "Tshiftback = np.float32([[1, 0, (I.shape[1] - 1) / 2.0], \n",
    "                         [0, 1, (I.shape[0] - 1) / 2.0], \n",
    "                         [0, 0, 1]])\n",
    "T = np.matmul(Tshiftback, np.matmul(Trotate, Tshift))[0:2, :]\n",
    "\n",
    "Irotate2 = cv.warpAffine(I, T, I.shape[:2][::-1])\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), (\"Image rotation\", Irotate2)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the OpenCV `getRotationMatrix2D()` function can be used to calculate the rotation matrix for counter-clockwise rotation of an image around arbitrary point on an arbitrary angle and scaling.\n",
    "\n",
    "Let's try it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image rotation around image center\n",
    "phi = 17.0 # here we use degrees to define an angle\n",
    "T = cv.getRotationMatrix2D(((I.shape[1] - 1) / 2.0, (I.shape[0] - 1) / 2.0), -phi, 1)\n",
    "Irotate_ocv = cv.warpAffine(I, T, I.shape[:2][::-1])\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), (\"Built-in image rotation\", Irotate_ocv)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Rotate an image around the center of the bottom left quarter with coordinates $(\\dfrac{1}{4} \\cdot width, \\dfrac{3}{4} \\cdot height)$ by $45\\degree$ clockwise. Implement two solutions:\n",
    ">\n",
    "> 1. Create the rotation matrix manually with three matrices;\n",
    "> 2. Use the OpenCV built-in `getRotationMatrix2D()` function.\n",
    ">\n",
    "> Compare the results, they should match\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution for part 1 here\n",
    "# Create matrices for shift, rotation and shift back\n",
    "# Multiply these matrices\n",
    "# Apply transformation with cv2.warpAffine() function\n",
    "# Display the result\n",
    "Irotate45 = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution for part 2 here\n",
    "# Create the rotation matrix with cv2.getRotationMatrix2D() function\n",
    "# Apply transformation with cv2.warpAffine() function\n",
    "# Display the result\n",
    "Irotate45_ocv = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your rotation results match? Don't forget to check it. There might be a little difference due to a different floating point values processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.5 Arbitrary affine transformation\n",
    "\n",
    "**Affine mapping** is a mapping, in which parallel lines go into parallel lines, intersect lines into intersect lines, cross lines into cross lines; the segments lengths ratios of lying on the one straight line (or on parallel lines) are preserved, and the figures areas ratios are preserved also. Basic transformations are linear transformations, bevel and non-uniform scaling. An arbitrary affine transformation can be obtained using the sequential product of the basic transformation matrices. In continuous geometry, any affine transformation has an inverse affine transformation, and the product of the direct and inverse transformations gives a unit transformation that leaves all points in place without changing. Affine transformations are a subset of projection transformations.\n",
    "\n",
    "| ![Affine mapping](images/affine.png \"Affine mapping\") | \n",
    "|:--:| \n",
    "| *Affine mapping* |\n",
    "\n",
    "\n",
    "Such transformation can be defined by three pairs of points and then the transformation matrix will be calculated from a linear equations system. Let us assume we have three pairs of matching points: $P_{src} = \\lbrace(x_1, y_1), (x_2, y_2), (x_3, y_3)\\rbrace$ which matches $P_{dst} = \\lbrace(x'_1, y'_1), (x'_2, y'_2), (x'_3, y'_3)\\rbrace$. Then after substitution to the transformation equations we will get:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "    x'_1 = x_1 \\cdot A + y_1 \\cdot B + C \\\\\n",
    "    y'_1 = x_1 \\cdot D + y_1 \\cdot E + F \\\\\n",
    "    x'_2 = x_2 \\cdot A + y_2 \\cdot B + C \\\\\n",
    "    y'_2 = x_2 \\cdot D + y_2 \\cdot E + F \\\\\n",
    "    x'_3 = x_3 \\cdot A + y_3 \\cdot B + C \\\\\n",
    "    y'_3 = x_3 \\cdot D + y_3 \\cdot E + F\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Which gives us six linear equations with 6 variables. Solving these equations will give us the transformation matrix. To do it, the `getAffineTransform()` function can be used, it takes 3 pairs of points as arguments and returns the corresponding transformation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the transformation matrix\n",
    "Psrc = np.float32([[50, 300], [150, 200], [50, 50]]) \n",
    "Pdst = np.float32([[50, 200], [250, 200], [50, 100]]) \n",
    "T = cv.getAffineTransform(Psrc, Pdst) \n",
    "Iaffine = cv.warpAffine(I, T, I.shape[:2][::-1]) \n",
    "\n",
    "# Draw triangles on source and transformed images\n",
    "Icopy = I.copy()\n",
    "Iaffine_copy = Iaffine.copy()\n",
    "cv.polylines(Icopy, [Psrc.astype(np.int32)], True, (0, 0, 0), 1)\n",
    "cv.polylines(Iaffine_copy, [Pdst.astype(np.int32)], True, (0, 0, 0), 1)\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", Icopy), (\"Affine mapping\", Iaffine_copy)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Implement the backward transformation of the `Iaffine` image. The transformation should use `Pdst` points as a source data and `Psrc` points as the destination. Transformed image should match the source image, however some areas may be black due to a data loss.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n",
    "# Calculate the transformation matrix for transformation from Pdst to Psrc\n",
    "# Apply transformation with cv2.warpAffine() function\n",
    "# Display the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.6 Image bevel\n",
    "\n",
    "The transformation equation and coordinates transformation matrix $T$ parameters in case of *image bevel* by factor $s$ along $oX$ axis are $A=E=1$, $B=s$, $C=D=F=0$. Then equations and matrix will take the form:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tx'=x+sy, \\\\\n",
    "\t\ty'=y,\n",
    "\t\\end{cases}\n",
    "\t\\Rightarrow \n",
    "\tT=\n",
    "\t\\begin{bmatrix} \n",
    "\t\t1 & 0 & 0 \n",
    "\t\t\\\\ s & 1 & 0\n",
    "\t\t\\\\ 0 & 0 & 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With OpenCV this will give us the following transformation matrix:\n",
    "\n",
    "$$\n",
    "\tT=\n",
    "\t\\begin{bmatrix} \n",
    "\t\t1 & 0 & 0 \n",
    "\t\t\\\\ s & 1 & 0\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We know how to implement it. Let's do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Implement the image bevel transformation according with the matrix above with $s = 0.3$ and run it.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n",
    "Ibevel = np.zeros_like(I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.7 Piecewise linear mapping\n",
    "\n",
    "In some cases we don't need to transform the whole image. *Piecewise-Linear Mapping* is a mapping in which the image is split into parts, and then various linear transformations are applied to each of these parts separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using OpenCV the piecewise operation can be performed with creation of ROI (Region of Interest) image. This ROI type of image shares the data with initial one but can be used as a completely separate image. In Python the ROI for an image is implemented using numpy array indexing and slicing.\n",
    "\n",
    "Let's try using piecewise linear mapping to scale the right half of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the transformation matrix\n",
    "stretch = 2\n",
    "T = np.float32([[stretch, 0, 0], \n",
    "                [0,       1, 0]])\n",
    "\n",
    "# Split and display image parts\n",
    "Ileft = I[:, 0:int(I.shape[1] / 2), :]\n",
    "Iright = I[:, int(I.shape[1] / 2):, :]\n",
    "ShowImages([(\"Left part\", Ileft), (\"Right part\", Iright)], 2)\n",
    "\n",
    "# Transform right part and stitch\n",
    "Iright = cv.warpAffine(Iright, T, (int(Iright.shape[1] * stretch), Iright.shape[0]))\n",
    "Iplm = np.concatenate((Ileft, Iright), axis = 1) \n",
    "\n",
    "# Display the result\n",
    "ShowImages([(\"Source image\", I), \n",
    "            (\"Piecewise linear mapping\", Iplm)], 2, hide_axes = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Implement the piecewise linear mapping transformation and scale the right part of an image with scale factor $0.5$.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n",
    "# Fill the transformation matrix\n",
    "# Split and display image parts\n",
    "# Transform right part and stitch\n",
    "# Display the result\n",
    "Iplm2 = np.zeros_like(I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Nonlinear transformations\n",
    "\n",
    "When considering geometric transformations, it is assumed that the images were obtained using an ideal camera model. In fact, imaging is accompanied by various nonlinear distortions. So, the nonlinear functions are used to correct them.\n",
    "\n",
    "| ![Nonlinear distortion examples](images/nonlinear.png \"Nonlinear distortion examples\") | \n",
    "|:--:| \n",
    "| *Nonlinear distortion examples* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Perspective mapping\n",
    "\n",
    "**Perspective mapping** is a mapping, in which straight lines remain straight lines, however, the figure geometry may be distorted, because this mapping generally does not preserve the lines parallelism. The property preserved under this mapping is points *collinearity*: three points lying on one straight line (collinear) remain on one straight line after mapping. Projection mapping can be as *parallel* (scale is changed), and *perspective* (the figure geometry is changed). \n",
    "\n",
    "| ![Perspective mapping](images/projective.png \"Perspective mapping\") | \n",
    "|:--:| \n",
    "| *Perspective mapping: straight lines remain straight, but parallel ones may intersect* |\n",
    "\n",
    "In the case of a perspective mapping a three-dimensional scene point $\\mathbf{P}^3$ projected onto a two-dimensional image plane $\\mathbf{P}^2$. This mapping $\\mathbf{P}^3 \\to \\mathbf{P}^2$ maps the scene Euclidean point $P=(x,y,z)$ (in homogeneous coordinates $(x',y',z',w')$) in the image point $X=(x,y)$ (in homogeneous coordinates $(x',y',w')$). To find the points Cartesian coordinates from homogeneous coordinates, we use the following relations: $P=(\\dfrac{x'}{w'}, \\dfrac{y'}{w'}, \\dfrac{z'}{w'})$ --- are coordinates of the vector $\\vec P$ and $X=(\\dfrac{x'}{w'}, \\dfrac{y'}{w'})$ ---  are coordinates of the vector $\\vec X$. Substituting in~(\\ref{baseTransformHomo}) $w=1$ for vector $\\vec X$ we obtain the equations system:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tx'=\\dfrac{Ax+By+C}{Gx+Hy+I}, \\\\\n",
    "\t\ty'=\\dfrac{Dx+Ey+F}{Gx+Hy+I},\n",
    "\t\\end{cases}\n",
    "\t\\Rightarrow \n",
    "\tT=\n",
    "\t\\begin{bmatrix} \n",
    "\t\tA & B & C \n",
    "\t\t\\\\ D & E & F\n",
    "\t\t\\\\ G & H & 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Due to the coordinates normalization to $w'$ in general, the projection mapping is nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With OpenCV we can define projective transformation matrix same as we did for the affine transformtaion, then use the `cv.warpPerspective()` function to apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perspective transformation\n",
    "T = np.float32([[1.1, 0.35, 0], [0.2, 1.1, 0], [0.00075, 0.0005, 1]])\n",
    "Ipersp = cv.warpPerspective(I, T, I.shape[:2][::-1])\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), (\"Perspective transformation\", Ipersp)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV also provides a handy `cv.getPerspectiveTransform()` function for the projective transformation matrix calculation for an arbitrary projection mapping can be calculated by specifying the coordinates of four source points $P_{srd} = \\lbrace(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4)\\rbrace$ and their coordinates after transformation $P_{dst} = \\lbrace(x'_1, y'_1), (x'_2, y'_2), (x'_3, y'_3), (x'_4, y'_4)\\rbrace$. It works a way similar as calculation of an affine trnasformation matrix: solves the system of linear equations.\n",
    "\n",
    "Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perspective transformation\n",
    "Psrc = np.float32([[50, 461], [461, 461], [461, 50], [50, 50]]) \n",
    "Pdst = np.float32([[50, 461], [461, 440], [450, 10], [100, 50]]) \n",
    "T = cv.getPerspectiveTransform(Psrc, Pdst) \n",
    "Ipersp2 = cv.warpPerspective(I, T, I.shape[:2][::-1])\n",
    "\n",
    "# Draw points on source and transformed images\n",
    "Icopy = I.copy()\n",
    "cv.polylines(Icopy, [Psrc.astype(np.int32)], True, (0, 0, 0), 1)\n",
    "cv.polylines(Ipersp2, [Pdst.astype(np.int32)], True, (0, 0, 0), 1)\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", Icopy), (\"Built-in perspective transformation\", Ipersp2)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Implement the backward perspective transformation of the `Ipersp2` image. The transformation should use `Pdst` points as a source data and `Psrc` points as the destination. Transformed image should match the source image, however some areas may be black due to a data loss.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n",
    "# Calculate the perspective transformation matrix for transformation from Pdst to Psrc\n",
    "# Apply the perspective transformation with cv2.warpPerspective() function\n",
    "# Display the result\n",
    "Ipersp3 = np.zeros_like(I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Polynomial mapping\n",
    "\n",
    "**Polynomial mapping** is an original image mapping using polynomials. In this case, the coordinate transformation matrix $T$ will contain the polynomials coefficients of the corresponding orders for the coordinates $x$ and $y$. For example, in case of polynomial mapping of the *second order* equations system takes the following form:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tx'=a_1+a_2x+a_3y+a_4x^2+a_5xy+a_6y^2 \\\\\n",
    "\t\ty'=b_1+b_2x+b_3y+b_4x^2+b_5xy+b_6y^2\n",
    "\t\\end{cases}\n",
    "$$\n",
    "\n",
    "Where $x$, $y$ are the point coordinates in one coordinate system, and $x'$, $y'$ are points coordinates in another coordinate system, $a_1 \\dots a_6$, $b_1 \\dots b_6$ are the mapping coefficients. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV has no built-in function for such kind of transformatios, and indexing array pixel by pixel is very ineffective when wring program with Python programming language. Instead of this two arrays for source and destination indexes should be calculated to copy data from one array to another in a single command. To create re-indexing arrays with Python, a `numpy.meshgrid()` array constructor is used. It creates two arrays to store $X$ and $Y$ coordinates for each image pixel. Then these coordinates are transformed to be used when mapping new image pixels with old ones. The `mask` is used to exclude pixels that became out of range after transformation.\n",
    "\n",
    "Let's implement a polynomial mapping with a given transformation coefficient as a matrix $T$ using OpenCV:\n",
    "\n",
    "$$x' = 0 + 1 \\cdot x + 0 \\cdot y + 0.00001 \\cdot x^2 + 0.002 \\cdot x y + 0.001 \\cdot y^2$$\n",
    "$$y' = 0 + 0 \\cdot x + 1 \\cdot y + 0 \\cdot x^2 + 0 \\cdot x y + 0 \\cdot y^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial mapping\n",
    "T = np.array([[0, 0], [1, 0], [0, 1], [0.00001, 0],\t[0.002, 0], [0.001, 0]])\n",
    "# x' = a0 + a1 x + a2 y + a3 x^2 + a4 xy + a5 y^2 \n",
    "# y' = b0 + b1 x + b2 y + b3 x^2 + b4 xy + b5 y^2\n",
    "# where\n",
    "# a = T[:, 0]\n",
    "# b = T[:, 1]\n",
    "\n",
    "# Create mesh grid for X, Y coordinates\n",
    "x, y = np.meshgrid(np.arange(I.shape[1]), np.arange(I.shape[0]))\n",
    "\n",
    "# Calculate new coordinates for X, Y\n",
    "x_new = np.round(T[0, 0] + x * T[1, 0] + y * T[2, 0] + x ** 2 * T[3, 0] + x * y * T[4, 0] + y ** 2 * T[5, 0]).astype(np.float32)\n",
    "y_new = np.round(T[0, 1] + x * T[1, 1] + y * T[2, 1] + x ** 2 * T[3, 1] + x * y * T[4, 1] + y ** 2 * T[5, 1]).astype(np.float32)\n",
    "\n",
    "# Create a mask where new coordinates are valid\n",
    "mask = np.logical_and(np.logical_and(x_new >= 0, x_new < I.shape[1]), \n",
    "                      np.logical_and(y_new >= 0, y_new < I.shape[0]))\n",
    "\n",
    "# Apply new coordinates\n",
    "Ipoly = np.zeros(I.shape, I.dtype)\n",
    "if I.ndim == 2:\n",
    "  Ipoly[y_new[mask].astype(int), x_new[mask].astype(int)] = I[y[mask], x[mask]]\n",
    "else:\n",
    "  Ipoly[y_new[mask].astype(int), x_new[mask].astype(int), :] = I[y[mask], x[mask], :]\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), (\"Polynomial mapping\", Ipoly)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not every pixel on the new image has the corresponding pixel color value the black areas appeared. It can be converted by interpolating the missing pixel values using the known ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Implement the polynomial mapping of the source image with following formula:\n",
    ">\n",
    "> $x' = 0 + 1 \\cdot x + 0 \\cdot y + 0 \\cdot x^2 + 0 \\cdot x y + 0 \\cdot y^2$\n",
    ">\n",
    "> $y' = 0 + 0 \\cdot x + 1 \\cdot y + 0.001 \\cdot x^2 + 0.002 \\cdot x y + 0.00001 \\cdot y^2$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n",
    "# Fill the polynomial mapping parameters array\n",
    "# Create mesh grid for X, Y coordinates\n",
    "# Calculate new coordinates for X, Y according to formula\n",
    "# Create a mask where new coordinates are valid\n",
    "# Apply new coordinates\n",
    "# Display the result\n",
    "Ipoly2 = np.zeros_like(I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Sinusoidal distortion\n",
    "\n",
    "Harmonic distortion of an image can be considered as another example of nonlinear transformation.\n",
    "\n",
    "Let's try creating the sinusoidal distortion. For this we will use OpenCV remapping feature whith allows to remap the whole image by defining the pixel coordinates in the source coordinate system for each pixel of the transformed image. This is implemented by the `remap()` function which transforms image according to new pixel coordinates specified for each pixel of the source image.\n",
    "\n",
    "Let's apply the sinusoidal distortion along $Ox$ axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mesh grid for X, Y coordinates\n",
    "x, y = np.meshgrid(np.arange(I.shape[1]), np.arange(I.shape[0]))\n",
    "\n",
    "# Distort the X coordinate depending on Y\n",
    "x = x + 20 * np.sin(2 * math.pi * y / 90)\n",
    "\n",
    "# Remap with new coordinates\n",
    "Isin_x = cv.remap(I, x.astype(np.float32), y.astype(np.float32), cv.INTER_LINEAR)\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), (\"Sinusoidal distortion Ox\", Isin_x)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Implement the sinusoidal distortion along $Oy$ axis with same parameter values.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n",
    "# Create mesh grid for X, Y coordinates\n",
    "# Distort the Y coordinate depending on X\n",
    "# Remap with new coordinates\n",
    "# Display the result\n",
    "Isin_y = np.zeros_like(I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Distortion correction\n",
    "\n",
    "*Select an arbitrary image with either pincushion or barrel distortion. Correct the image.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the optical system is forming an image, *distortion* may occur on the image. \n",
    "\n",
    "**Distortion** is an optical distortion that is expressed in the curvature of straight lines. Light rays passing through the center of the lens converge at a point farther from the lens than rays passing through the edge of the lens. Straight lines are curved, except for those that lie in the same plane with the optical axis. For example, the image of a square, the center of which intersects the optical axis, looks like a pillow (pincushion distortion*) with positive distortion and looks like a barrel (*barrel distortion*) with negaitive distortion.\n",
    "\n",
    "| ![Distortions examples](images/distortions.png \"Distortions examples\") | \n",
    "|:--:| \n",
    "| *Distortions examples. Left is the original image, center is a pincushion distortion, right is a barrel distortion* |\n",
    "\n",
    "Let $\\vec{r}=(x,y)$ --- vector specifying two coordinates in a plane perpendicular to the optical axis. For an ideal image, all rays leaving this point and passing through the optical system will hit the image point with coordinates $\\vec{R}$, which are determined by the formula:\n",
    "\n",
    "$$\n",
    "\t\\vec{R}=b_0\\vec{r}\n",
    "$$\n",
    "\n",
    "Where $b_0$ is a linear increasing parameter.\n",
    "\n",
    "If distortion of a higher order is present (for axisymmetric optical systems, the distortion can only be of odd orders: third, fifth, seventh, etc.), then it is necessary to add the appropriate terms:\n",
    "\n",
    "$$\n",
    "\t\\vec{R}=b_0\\vec{r}+F_3r^2\\vec{r}+F_5r^4\\vec{r}+\\dots\n",
    "$$\n",
    "\n",
    "where $r$ is the vector length $\\vec{r}$; $F_i, i=3,5,\\dots,n$ are the distortion coefficients of $n$-th order, which contribute the most to the distortion of the image shape. With third-order distortion, if the coefficient $F_3$ has the same sign as $b_0$: $sign(F_3)=sign(b_0)$, pincushion distortion occurs, otherwise a barrel distortion occurs. \n",
    "\n",
    "To correct the distortion, the approach described above for affine of projective transformation is used. An image of a regular grid and its distorted image is used, pairs of points on these images are found, and the coefficients of the corrective transformation are calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Barrel distortion\n",
    "\n",
    "To distort an image we will do the following steps:\n",
    "\n",
    "1. First, create a `meshgrid` for our image pixel coordinates;\n",
    "2. Then shift the mesh coordinates so that image center has coordinates $(0, 0)$ and transform them to a polar coordinate system;\n",
    "3. Apply distortion transformation in the polar coordinate system;\n",
    "4. Convert mesh coordinates back to cartesian coordinate system and shift back;\n",
    "5. Remap an image with the calculated mesh map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's distort an image according to formula:\n",
    "$$\\vec{R}=\\vec{r} + 0.1 \\cdot r^2\\vec{r} + 0.12 \\cdot r^4\\vec{r}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mesh grid for X, Y coordinates\n",
    "x, y = np.meshgrid(np.arange(I.shape[1]), np.arange(I.shape[0]))\n",
    "\n",
    "mid = I.shape[1] / 2.0, I.shape[0] / 2.0\n",
    "x, y = x - mid[0], y - mid[1]\n",
    "\n",
    "r, theta = cv.cartToPolar(x / mid[0], y / mid[1])\n",
    "b0 = 1\n",
    "F3 = 0.1\n",
    "F5 = 0.12\n",
    "r = b0 * r + F3 * r ** 3 + F5 * r ** 5\n",
    "u, v = cv.polarToCart(r, theta)\n",
    "u = u * mid[0] + mid[0]\n",
    "v = v * mid[1] + mid[1]\n",
    "\n",
    "Ibarrel = cv.remap(I, u.astype(np.float32), v.astype(np.float32), cv.INTER_LINEAR)\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), (\"Barrel distortion\", Ibarrel)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To correct the distortion we need to define a set of matching points on both distored and corrected image. We will calculate these points along radius in polar coordinate system assuming they were defined by user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the set of matching points\n",
    "d = 5  # Number of points\n",
    "max_r = 1.3  # points array scale\n",
    "Psrc = (np.arange(d, dtype = np.float32) + 1) / d * max_r\n",
    "Pdst = b0 * Psrc + F3 * Psrc ** 3 + F5 * Psrc ** 5\n",
    "\n",
    "# Now create and fill matrix for linear equations\n",
    "T = np.zeros((d, d))\n",
    "for i in range(d):\n",
    "  T[:, i] = Pdst ** (i + 1)\n",
    "# Invert it and calculate coefficients\n",
    "T = np.linalg.inv(T)\n",
    "F = np.matmul(T, Psrc)\n",
    "\n",
    "# Now use the calculated parameters to calculate the new mapping\n",
    "# x and y are already shifted, so we don't need to repeat it\n",
    "r, theta = cv.cartToPolar(x / mid[0], y / mid[1])\n",
    "rt = 0\n",
    "for i in range(d):\n",
    "  rt = rt + F[i] * r ** (i + 1)\n",
    "r = rt\n",
    "\n",
    "# Now convert back to cartesian\n",
    "u, v = cv.polarToCart(r, theta)\n",
    "u = u * mid[0] + mid[0];\n",
    "v = v * mid[1] + mid[1];\n",
    "\n",
    "# Apply and display\n",
    "Ibarrel_corr = cv.remap(Ibarrel, u.astype(np.float32), v.astype(np.float32), cv.INTER_LINEAR)\n",
    "ShowImages([(\"Barrel distortion\", Ibarrel), (\"Distortion correction\", Ibarrel_corr)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Distort an image according with formula $\\vec{R}=\\vec{r} - 0.2 \\cdot r^2\\vec{r} + 0.32 \\cdot r^4\\vec{r}$ and correct the distortion.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pincushion distortion\n",
    "\n",
    "The pincushion distortion is calculated a similar way but with different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's distort an image according to formula:\n",
    "$$\\vec{R}=1.3 \\cdot \\vec{r} - 0.15 \\cdot r^2\\vec{r}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mesh grid for X, Y coordinates\n",
    "x, y = np.meshgrid(np.arange(I.shape[1]), np.arange(I.shape[0]))\n",
    "\n",
    "mid = I.shape[1] / 2.0, I.shape[0] / 2.0\n",
    "x, y = x - mid[0], y - mid[1]\n",
    "\n",
    "r, theta = cv.cartToPolar(x / mid[0], y / mid[1])\n",
    "b0 = 1.2\n",
    "F3 = -0.15\n",
    "F5 = 0\n",
    "r = b0 * r + F3 * r ** 3 + F5 * r ** 5\n",
    "u, v = cv.polarToCart(r, theta)\n",
    "u = u * mid[0] + mid[0]\n",
    "v = v * mid[1] + mid[1]\n",
    "\n",
    "Ipincushion = cv.remap(I, u.astype(np.float32), v.astype(np.float32), cv.INTER_LINEAR)\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I), (\"Pincushion distortion\", Ipincushion)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's correct this one as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the set of matching points\n",
    "d = 5  # Number of points\n",
    "max_r = 1.4  # points array scale\n",
    "Psrc = (np.arange(d, dtype = np.float32) + 1) / d * max_r\n",
    "Pdst = b0 * Psrc + F3 * Psrc ** 3 + F5 * Psrc ** 5\n",
    "\n",
    "# Now create and fill matrix for linear equations\n",
    "T = np.zeros((d, d))\n",
    "for i in range(d):\n",
    "  T[:, i] = Pdst ** (i + 1)\n",
    "# Invert it and calculate coefficients\n",
    "T = np.linalg.inv(T)\n",
    "F = np.matmul(T, Psrc)\n",
    "\n",
    "# Now use the calculated parameters to calculate the new mapping\n",
    "# x and y are already shifted, so we don't need to repeat it\n",
    "r, theta = cv.cartToPolar(x / mid[0], y / mid[1])\n",
    "rt = 0\n",
    "for i in range(d):\n",
    "  rt = rt + F[i] * r ** (i + 1)\n",
    "r = rt\n",
    "\n",
    "# Now convert back to cartesian\n",
    "u, v = cv.polarToCart(r, theta)\n",
    "u = u * mid[0] + mid[0];\n",
    "v = v * mid[1] + mid[1];\n",
    "\n",
    "# Apply and display\n",
    "Ipincushion_corr = cv.remap(Ipincushion, u.astype(np.float32), v.astype(np.float32), cv.INTER_LINEAR)\n",
    "ShowImages([(\"Pincushion distortion\", Ipincushion), (\"Distortion correction\", Ipincushion_corr)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen this image is also corrected. Black areas appeared because of data loss when the distortion was applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Distort an image according with formula $\\vec{R}=1.3 \\cdot \\vec{r} - 0.2 \\cdot r^2\\vec{r} + 0.05 \\cdot r^4\\vec{r}$ and correct the distortion.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Stitching images\n",
    "\n",
    "*Select two images (photographs from a cam- era, fragments of a scanned image, etc.) which have an overlapping area. Correct the second image to translate it into the coordinate system of the first one; then perform automatic glueing from two images into the one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometric transformations can be used, for example, to create mosaics from multiple images. Mosaic \"*stitching*\", \"*gluing*\") is the combination of two or more images into a single one, and the images coordinate systems  being glued may differ due to different shooting angles, changes in the camera position or the object movement. However, it is necessary that both images have overlapping areas, i.e. they were attended by the same objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Manual image stitching\n",
    "\n",
    "The main task of processing such images is to bring them into a common coordinate system. As a common coordinate system, you can use the system of the first image, then you need to find the coordinate transformation of all pixels of the second image $(x,y)$ to the common coordinate system $(x',y')$. If there is a projective distortion, then to recalculate the coordinates, you can use the projective transformation. In the case of an affine mapping, the transformation takes the form:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tx'=a_1+a_2x+a_3y, \\\\\n",
    "\t\ty'=b_1+b_2x+b_3\n",
    "\t\\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore, it is only necessary to find only 3 parameters for each coordinate:\n",
    "\n",
    "$$\n",
    "\t\\begin{bmatrix}\n",
    "\t\ta_1  \\\\ a_2 \\\\ a_3\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & x_1 & y_1\n",
    "\t\t\\\\ 1 & x_2 & y_2\n",
    "\t\t\\\\ 1 & x_3 & y_3\n",
    "\t\\end{bmatrix}^{-1}\n",
    "\t\\begin{bmatrix}\n",
    "\t\tx'_1  \\\\ x'_2 \\\\ x'_3\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\t\\begin{bmatrix}\n",
    "\t\tb_1  \\\\ b_2 \\\\ b_3\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & x_1 & y_1\n",
    "\t\t\\\\ 1 & x_2 & y_2\n",
    "\t\t\\\\ 1 & x_3 & y_3\n",
    "\t\\end{bmatrix}^{-1}\n",
    "\t\\begin{bmatrix}\n",
    "\t\ty'_1  \\\\ y'_2 \\\\ y'_3\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To do this we will search for a correlation between two images. We will take severla bottom rows of the first image and serach for a correlation of these rows in a second image.\n",
    "\n",
    "In OpenCV the correlation between two images and it is calculated with `matchTemplate()` function. This functions checks correlation with a given template over the whole image and returns an image with size $I.size - template.size$ where foe the `TM_CCOEFF` mode the higher the value is the higher is the correlation rate when matching a template from this point. We need to select the highest correlation point and shift the second image by this value before merging using the ROI image indexing.\n",
    "\n",
    "Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_top = \"images/lena_top.png\";\n",
    "fn_bottom = \"images/lena_bottom.png\";\n",
    "\n",
    "# Read an images from files\n",
    "Itop = cv.imread(fn_top, cv.IMREAD_COLOR)\n",
    "if not isinstance(Itop, np.ndarray) or Itop.data == None:\n",
    "  print(\"Error reading file \\\"{}\\\"\".format(fn_top))\n",
    "Ibottom = cv.imread(fn_bottom, cv.IMREAD_COLOR)\n",
    "if not isinstance(Ibottom, np.ndarray) or Ibottom.data == None:\n",
    "  print(\"Error reading file \\\"{}\\\"\".format(fn_bottom))\n",
    "\n",
    "# Show parts of image\n",
    "ShowImages([(\"Top part\", Itop), (\"Bottom part\", Ibottom)], 1)\n",
    "\n",
    "# Match template\n",
    "templ_size = 10\n",
    "templ = Itop[-templ_size:, :, :]\n",
    "res = cv.matchTemplate(Ibottom, templ, cv.TM_CCOEFF)\n",
    "min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)\n",
    "\n",
    "# Stitch images\n",
    "Istitch = np.zeros((Itop.shape[0] + Ibottom.shape[0] - max_loc[1] - templ_size, \n",
    "                    Itop.shape[1], Itop.shape[2]), dtype = np.uint8)\n",
    "Istitch[0:Itop.shape[0], :, :] = Itop\n",
    "Istitch[Itop.shape[0]:, :, :] = Ibottom[max_loc[1] + templ_size:, :, :]\n",
    "\n",
    "# Show stitched image\n",
    "ShowImages([(\"Stitched image\", Istitch)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Take some image, split it into two parts and try stitching it. You can split an image with OpenCV as well and then stitch it back:\n",
    "> ```Python\n",
    "> Itop = I[0:int(I.shape[0] / 2) + 20, :, :]\n",
    "> Ibottom = I[int(I.shape[0] / 2):, :, :]\n",
    "> ```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Automatic image stitching\n",
    "\n",
    "OpenCV library provides a special class for automatic stitching of images called `Stitcher`. It is designed to work either for stitching panoramic photos `cv2.Stitcher_PANORAMA` mode or for stitching scanned images `cv2.Stitcher_SCANS` mode. Working with `Stitcher` object is very simple. First, need to create an object. Then need to create an array of OpenCV images and pass them to the `stitch()` method. It returns a tuple with its status and a stitched image. The status value shows wether the stitching succeeded. In case of success it is equal to `cv2.Stitcher_OK`.\n",
    "\n",
    "**Please note** that this method requires at least tree images to work correctly.\n",
    "\n",
    "Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_top = \"images/lena_top.png\"\n",
    "fn_mid = \"images/lena_mid.png\"\n",
    "fn_bottom = \"images/lena_bottom.png\"\n",
    "\n",
    "# Read an images from files\n",
    "Itop = cv.imread(fn_top, cv.IMREAD_COLOR)\n",
    "if not isinstance(Itop, np.ndarray) or Itop.data == None:\n",
    "  print(\"Error reading file \\\"{}\\\"\".format(fn_top))\n",
    "Imid = cv.imread(fn_mid, cv.IMREAD_COLOR)\n",
    "if not isinstance(Imid, np.ndarray) or Imid.data == None:\n",
    "  print(\"Error reading file \\\"{}\\\"\".format(fn_mid))\n",
    "Ibottom = cv.imread(fn_bottom, cv.IMREAD_COLOR)\n",
    "if not isinstance(Ibottom, np.ndarray) or Ibottom.data == None:\n",
    "  print(\"Error reading file \\\"{}\\\"\".format(fn_bottom))\n",
    "\n",
    "# Show parts of image\n",
    "ShowImages([(\"Top part\", Itop), (\"Mid part\", Imid), (\"Bottom part\", Ibottom)], 1)\n",
    "\n",
    "# Stitch\n",
    "stitcher = cv.Stitcher.create(cv.Stitcher_SCANS) \n",
    "status, Istitch2 = stitcher.stitch([Itop, Imid, Ibottom])\n",
    "\n",
    "# Show stitched image in case of success\n",
    "if status == cv.Stitcher_OK:\n",
    "  ShowImages([(\"Stitched image\", Istitch2)])\n",
    "else:\n",
    "  print(\"Stitching error occurred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This OpenCV stitcher object uses more complex stitching algorithm that is based on feature points matching between images, so may not work well for too simple cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ***Self-work***\n",
    ">\n",
    "> Answer questions and conclude your work results.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Questions\n",
    "\n",
    "Please answer the following questions:\n",
    "\n",
    " - How can you rotate an image without using a rotation matrix?\n",
    " > Put your answer here\n",
    " \n",
    " - What is the minimum number of corresponding pairs of points that must be specified on the original and distorted images to correct a perspective distortion?\n",
    " > Put your answer here\n",
    "\n",
    " - After geometric transformation of the image, pixels with undefined intensity values may appear. What is the reason for this and how can this problem be solved?\n",
    " > Put your answer here\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "What have you learned with this task? Don't forget to conclude it.\n",
    " > Put your conclusion here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
